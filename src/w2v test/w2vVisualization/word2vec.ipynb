{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import nltk\n",
    "import re\n",
    "#nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from gensim import corpora, models, similarities\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                            abstract publish_time\n",
      "0  OBJECTIVE: This retrospective chart review des...   2001-07-04\n",
      "1  Inflammatory diseases of the respiratory tract...   2000-08-15\n",
      "2  Surfactant protein-D (SP-D) participates in th...   2000-08-25\n",
      "3  Endothelin-1 (ET-1) is a 21 amino acid peptide...   2001-02-22\n",
      "4  Respiratory syncytial virus (RSV) and pneumoni...   2001-05-11\n"
     ]
    }
   ],
   "source": [
    "file = 'newmeta.csv'\n",
    "\n",
    "new_data = pd.read_csv(file, low_memory=False)\n",
    "print(new_data.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                            abstract publish_time  \\\n",
      "0  OBJECTIVE: This retrospective chart review des...   2001-07-04   \n",
      "1  Inflammatory diseases of the respiratory tract...   2000-08-15   \n",
      "\n",
      "                                      tokenized_data  \n",
      "0  [objectivethisretrospectivechartreviewdescribe...  \n",
      "1  [inflammatorydiseasesoftherespiratorytractarec...  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\n# (3) CREATE GENSIM LIBRARY AND CORPUS\\n# gensim dictionary from tokenized data\\ntoken = new_data['tokenized_data']\\n\\n# dictionary will be used in corpus\\ndictionary = corpora.Dictionary(token)\\n\\n# filter keywords, we want the ones that show up the most in abstracts\\ndictionary.filter_extremes(no_below = 1, no_above = 0.8)         # filter keywords\\n\\n# dictionary to corpus\\ncorpus = [dictionary.doc2bow(tokens) for tokens in token]\\n\\n# prints the corpus for the first document\\n# corpus (1, 1) implies that the word with the id of 1 has occurred only once in the first document\\n# corpus (14, 4) implies that the word with the id of 14 has occurred 4 times in the first document\\nprint(corpus[:1])\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def clean(text):\n",
    "    text = re.sub(\"[^a-zA-Z]\", \"\", str(text))\n",
    "    text = text.lower()\n",
    "    text = nltk.word_tokenize(text)\n",
    "    return text\n",
    "\n",
    "# as seen in the first few runs of lda, these words contributed to the noise\n",
    "# when it came to topic modeling. To remove the noise, we'll remove these words\n",
    "# and get the relevant topics\n",
    "common_words = stopwords.words('english')\n",
    "common_words.extend(['of', 'and', 'the', 'in', 'were', 'to', 'nan', 'with'])\n",
    "def remove_words(text):\n",
    "  return [word for word in text if word not in common_words]\n",
    "\n",
    "# stemming helps reduce some of the noise too. It chops some words, but we still\n",
    "# know what the word says\n",
    "stemmer = PorterStemmer()\n",
    "def stem_words(text):\n",
    "  text = [stemmer.stem(word) for word in text]\n",
    "  return text\n",
    "\n",
    "# this function applies the stemmer, removal of common words, removal of punctuation,\n",
    "# lowercases the text, and tokenizes everything\n",
    "def preprocess(text):\n",
    "  return stem_words(remove_words(clean(text)))\n",
    "\n",
    "# token the data to be used with gensim, just looking at the abstracts\n",
    "new_data['tokenized_data'] = new_data['abstract'].apply(preprocess)\n",
    "\n",
    "# print the first two rows of the cleaned data\n",
    "# this will show the tokenized data from the abstract, basically splits up\n",
    "# the abstract into single words\n",
    "print(new_data.head(2))\n",
    "\n",
    "'''\n",
    "# (3) CREATE GENSIM LIBRARY AND CORPUS\n",
    "# gensim dictionary from tokenized data\n",
    "token = new_data['tokenized_data']\n",
    "\n",
    "# dictionary will be used in corpus\n",
    "dictionary = corpora.Dictionary(token)\n",
    "\n",
    "# filter keywords, we want the ones that show up the most in abstracts\n",
    "dictionary.filter_extremes(no_below = 1, no_above = 0.8)         # filter keywords\n",
    "\n",
    "# dictionary to corpus\n",
    "corpus = [dictionary.doc2bow(tokens) for tokens in token]\n",
    "\n",
    "# prints the corpus for the first document\n",
    "# corpus (1, 1) implies that the word with the id of 1 has occurred only once in the first document\n",
    "# corpus (14, 4) implies that the word with the id of 14 has occurred 4 times in the first document\n",
    "print(corpus[:1])'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[(0, 1)]]\n"
     ]
    }
   ],
   "source": [
    "# (3) CREATE GENSIM LIBRARY AND CORPUS\n",
    "# gensim dictionary from tokenized data\n",
    "token = new_data['tokenized_data']\n",
    "\n",
    "# dictionary will be used in corpus\n",
    "dictionary = corpora.Dictionary(token)\n",
    "\n",
    "# filter keywords, we want the ones that show up the most in abstracts\n",
    "dictionary.filter_extremes(no_below = 1, no_above = 0.8)         # filter keywords\n",
    "\n",
    "# dictionary to corpus\n",
    "corpus = [dictionary.doc2bow(tokens) for tokens in token]\n",
    "\n",
    "# prints the corpus for the first document\n",
    "# corpus (1, 1) implies that the word with the id of 1 has occurred only once in the first document\n",
    "# corpus (14, 4) implies that the word with the id of 14 has occurred 4 times in the first document\n",
    "print(corpus[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary(99039 unique tokens: ['objectivethisretrospectivechartreviewdescribestheepidemiologyandclinicalfeaturesofpatientswithcultureprovenmycoplasmapneumoniaeinfectionsatkingabdulazizuniversityhospitaljeddahsaudiarabiamethodspatientswithpositivempneumoniaeculturesfromrespiratoryspecimensfromjanuarythroughdecemberwereidentifiedthroughthemicrobiologyrecordschartsofpatientswerereviewedresultspatientswereidentifiedofwhomrequiredadmissionmostinfectionswerecommunityacquiredtheinfectionaffectedallagegroupsbutwasmostcommonininfantsandpreschoolchildrenitoccurredyearroundbutwasmostcommoninthefallandspringmorethanthreequartersofpatientshadcomorbiditiestwentyfourisolateswereassociatedwithpneumoniawithupperrespiratorytractinfectionsandwithbronchiolitiscoughfeverandmalaisewerethemostcommonsymptomsandcrepitationsandwheezeswerethemostcommonsignsmostpatientswithpneumoniahadcrepitationsbutonlyhadbronchialbreathingimmunocompromisedpatientsweremorelikelythannonimmunocompromisedpatientstopresentwithpneumoniaversuspofthepatientswithpneumoniahaduneventfulrecoveryrecoveredfollowingsomecomplicationsdiedbecauseofmpneumoniaeinfectionanddiedduetounderlyingcomorbiditiesthepatientswhodiedofmpneumoniaepneumoniahadothercomorbiditiesconclusionourresultsweresimilartopublisheddataexceptforthefindingthatinfectionsweremorecommonininfantsandpreschoolchildrenandthatthemortalityrateofpneumoniainpatientswithcomorbiditieswashigh', 'inflammatorydiseasesoftherespiratorytractarecommonlyassociatedwithelevatedproductionofnitricoxidenoandincreasedindicesofnodependentoxidativestressalthoughnoisknowntohaveantimicrobialantiinflammatoryandantioxidantpropertiesvariouslinesofevidencesupportthecontributionofnotolunginjuryinseveraldiseasemodelsonthebasisofbiochemicalevidenceitisoftenpresumedthatsuchnodependentoxidationsareduetotheformationoftheoxidantperoxynitritealthoughalternativemechanismsinvolvingthephagocytederivedhemeproteinsmyeloperoxidaseandeosinophilperoxidasemightbeoperativeduringconditionsofinflammationbecauseoftheoverwhelmingliteratureonnogenerationandactivitiesintherespiratorytractitwouldbebeyondthescopeofthiscommentarytoreviewthisareacomprehensivelyinsteaditfocusesonrecentevidenceandconceptsofthepresumedcontributionofnotoinflammatorydiseasesofthelung', 'surfactantproteindspdparticipatesintheinnateresponsetoinhaledmicroorganismsandorganicantigensandcontributestoimmuneandinflammatoryregulationwithinthelungspdissynthesizedandsecretedbyalveolarandbronchiolarepithelialcellsbutisalsoexpressedbyepithelialcellsliningvariousexocrineductsandthemucosaofthegastrointestinalandgenitourinarytractsspdacollagenouscalciumdependentlectinorcollectinbindstosurfaceglycoconjugatesexpressedbyawidevarietyofmicroorganismsandtooligosaccharidesassociatedwiththesurfaceofvariouscomplexorganicantigensspdalsospecificallyinteractswithglycoconjugatesandothermoleculesexpressedonthesurfaceofmacrophagesneutrophilsandlymphocytesinadditionspdbindstospecificsurfactantassociatedlipidsandcaninfluencetheorganizationoflipidmixturescontainingphosphatidylinositolinvitroconsistentwiththesediverseinvitroactivitiesistheobservationthatspddeficienttransgenicmiceshowabnormalaccumulationsofsurfactantlipidsandrespondabnormallytochallengewithrespiratoryvirusesandbacteriallipopolysaccharidesthephenotypeofmacrophagesisolatedfromthelungsofspddeficientmiceisalteredandthereiscircumstantialevidencethatabnormaloxidantmetabolismandorincreasedmetalloproteinaseexpressioncontributestothedevelopmentofemphysematheexpressionofspdisincreasedinresponsetomanyformsoflunginjuryanddeficientaccumulationofappropriatelyoligomerizedspdmightcontributetothepathogenesisofavarietyofhumanlungdiseas', 'endothelinetisaaminoacidpeptidewithdiversebiologicalactivitythathasbeenimplicatedinnumerousdiseasesetisapotentmitogenregulatorofsmoothmuscletoneandinflammatorymediatorthatmayplayakeyroleindiseasesoftheairwayspulmonarycirculationandinflammatorylungdiseasesbothacuteandchronicthisreviewwillfocusonthebiologyofetanditsroleinlungdiseas', 'respiratorysyncytialvirusrsvandpneumoniavirusofmicepvmarevirusesofthefamilyparamyxoviridaesubfamilypneumoviruswhichcauseclinicallyimportantrespiratoryinfectionsinhumansandrodentsrespectivelytherespiratoryepithelialtargetcellsrespondtoviralinfectionwithspecificalterationsingeneexpressionincludingproductionofchemoattractantcytokinesadhesionmoleculeselementsthatarerelatedtotheapoptosisresponseandothersthatremainincompletelyunderstoodherewereviewourcurrentunderstandingofthesemucosalresponsesanddiscussseveralgenomicapproachesincludingdifferentialdisplayreversetranscriptionpolymerasechainreactionpcrandgenearraystrategiesthatwillpermitustounravelthenatureoftheseresponsesinamorecompleteandsystematicmann']...)\n"
     ]
    }
   ],
   "source": [
    "model = gensim.models.word2vec(sentences=corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.test.utils import common_texts, get_tmpfile\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Load pre-trained Word2Vec model.\n",
    "model = gensim.models.Word2Vec.load(\"w2v.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sklearn in /usr/local/lib/python3.7/site-packages (0.0)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/site-packages (from sklearn) (0.23.1)\n",
      "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/site-packages (from scikit-learn->sklearn) (0.15.1)\n",
      "Requirement already satisfied: scipy>=0.19.1 in /usr/local/lib/python3.7/site-packages (from scikit-learn->sklearn) (1.4.1)\n",
      "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.7/site-packages (from scikit-learn->sklearn) (1.18.5)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/site-packages (from scikit-learn->sklearn) (2.1.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: plotly in /usr/local/lib/python3.7/site-packages (4.8.1)\n",
      "Requirement already satisfied: retrying>=1.3.3 in /usr/local/lib/python3.7/site-packages (from plotly) (1.3.3)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.7/site-packages (from plotly) (1.15.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install sklearn\n",
    "%pip install plotly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-f6fd4bc62f93>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m \u001b[0mx_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreduce_dimensions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mplot_with_plotly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplot_in_notebook\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import IncrementalPCA    # inital reduction\n",
    "from sklearn.manifold import TSNE                   # final reduction\n",
    "import numpy as np \n",
    "\n",
    "def reduce_dimensions(model):\n",
    "    num_dimensions = 2  # final num dimensions (2D, 3D, etc)\n",
    "\n",
    "    vectors = [] # positions in vector space\n",
    "    labels = [] # keep track of words to label our data again later\n",
    "    for word in model.wv.vocab:\n",
    "        vectors.append(model.wv[word])\n",
    "        labels.append(word)\n",
    "\n",
    "    # convert both lists into numpy vectors for reduction\n",
    "    vectors = np.asarray(vectors)\n",
    "    labels = np.asarray(labels)\n",
    "\n",
    "    # reduce using t-SNE\n",
    "    vectors = np.asarray(vectors)\n",
    "    tsne = TSNE(n_components=num_dimensions, random_state=0)\n",
    "    vectors = tsne.fit_transform(vectors)\n",
    "\n",
    "    x_vals = [v[0] for v in vectors]\n",
    "    y_vals = [v[1] for v in vectors]\n",
    "    return x_vals, y_vals, labels\n",
    "\n",
    "\n",
    "x_vals, y_vals, labels = reduce_dimensions(model)\n",
    "\n",
    "def plot_with_plotly(x_vals, y_vals, labels, plot_in_notebook=True):\n",
    "    from plotly.offline import init_notebook_mode, iplot, plot\n",
    "    import plotly.graph_objs as go\n",
    "\n",
    "    '''trace = go.Scatter(x=x_vals, y=y_vals, mode='text', text=labels)\n",
    "    data = [trace]\n",
    "\n",
    "    if plot_in_notebook:\n",
    "        init_notebook_mode(connected=True)\n",
    "        iplot(data, filename='word-embedding-plot')\n",
    "    else:\n",
    "        plot(data, filename='word-embedding-plot.html')\n",
    "\n",
    "\n",
    "def plot_with_matplotlib(x_vals, y_vals, labels):\n",
    "    import matplotlib.pyplot as plt\n",
    "    import random\n",
    "\n",
    "    random.seed(0)\n",
    "\n",
    "    plt.figure(figsize=(12, 12))\n",
    "    plt.scatter(x_vals, y_vals)\n",
    "\n",
    "    #\n",
    "    # Label randomly subsampled 25 data points\n",
    "    #\n",
    "    indices = list(range(len(labels)))\n",
    "    selected_indices = random.sample(indices, 25)\n",
    "    for i in selected_indices:\n",
    "        plt.annotate(labels[i], (x_vals[i], y_vals[i]))\n",
    "\n",
    "try:\n",
    "    get_ipython()\n",
    "except Exception:\n",
    "    plot_function = plot_with_matplotlib\n",
    "else:\n",
    "    plot_function = plot_with_plotly\n",
    "\n",
    "plot_function(x_vals, y_vals, labels)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
